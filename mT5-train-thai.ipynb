{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbwjJF8nAHrn"
      },
      "source": [
        "# Welcome to mt5-thai-QG\n",
        "This is notebook detailing how to finetune **mt5 for question-generation in the Thai language** \n",
        "\n",
        "\n",
        "First, we will mount our google drive so our models don't get deleted (╯°□°)╯︵ ┻━┻ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ecgcLxQBIym",
        "outputId": "b5f38d9e-66df-4df1-c25a-bd428722e5cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Pj_wDYBKS4"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Now we will install some requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy4QtOY3AHrq",
        "outputId": "0b3199f9-afd2-4bab-828e-ca5378e80c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ijson\n",
            "  Downloading ijson-3.1.4-cp37-cp37m-manylinux2010_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 23.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 56.0 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 64.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 59.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 14.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.0)\n",
            "Collecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Collecting PyYAML>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.46.3)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 75.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.6 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, tokenizers, pytorch-lightning, huggingface-hub, transformers, sentencepiece, lightning-bolts, ijson\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.7.0 ijson-3.1.4 lightning-bolts-0.5.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 sentencepiece-0.1.96 tokenizers-0.12.1 torchmetrics-0.9.1 transformers-4.19.4 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "# Remove \"sample_data\" in colab\n",
        "!rm -rf sample_data\n",
        "# Solve some protobuf problems\n",
        "!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='python'\n",
        "# Install programs\n",
        "\n",
        "## Install ONNX\n",
        "#%pip install torch-ort onnxruntime-training -f https://download.onnxruntime.ai/onnxruntime_stable_cu111.html\n",
        "#!apt install ninja-build\n",
        "#!python -m torch_ort.configure\n",
        "\n",
        "## Install other stuff\n",
        "%pip install ijson pandas torchmetrics lightning-bolts transformers sentencepiece protobuf beautifulsoup4 pytorch-lightning \n",
        "%pip install pythainlp epitran\n",
        "%pip install -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXaB7UrtCyNz"
      },
      "source": [
        "Then we will import all the things we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "thZlTjPEC2Hl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import urllib.request\n",
        "import os\n",
        "import ijson\n",
        "import json\n",
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "# from pl_bolts.callbacks import ORTCallback\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    MT5TokenizerFast,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1xgys6NAHrt"
      },
      "source": [
        "# Gather & Process datasets\n",
        "- xquad-thai\n",
        "- iapp-wiki-qa-dataset\n",
        "- thaiqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "CkNR3MnNAHrt",
        "outputId": "a3dc81b9-809c-4c7f-8115-6d851a33b6e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded xquad.json from https://github.com/deepmind/xquad/raw/master/xquad.th.json\n",
            "Downloaded iapp-thai-wikipedia-qa.json from https://raw.githubusercontent.com/iapp-technology/iapp-wiki-qa-dataset/main/iapp-thai-wikipedia-qa-1961-docs-9170-questions.json\n",
            "Downloaded thaiqa.zip from https://github.com/PyThaiNLP/thaiqa_squad/raw/main/data.zip\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4e09f753-e30b-4d76-b82f-d63e51da632e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>สร้าง 14 คำถาม: ﻿ทีมรับของแพนเธอร์สถอดใจที่คะแ...</td>\n",
              "      <td>1. ทีมรับของแพนเธอร์สยอมแพ้ที่คะแนนเท่าไร A: 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>สร้าง 16 คำถาม: ทีมบรอนคอส เอาชนะทีม พิตต์สเบิ...</td>\n",
              "      <td>1. ใครพ่ายแพ้ให้แก่ทีมบรอนคอสในรอบดิวิชั่น A: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>สร้าง 17 คำถาม: เพย์ตัน แมนนิง กลายเป็นควอเตอร...</td>\n",
              "      <td>1. เพย์ตัน แมนนิง อายุเท่าไรตอนที่เขาเล่นในซูเ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>สร้าง 12 คำถาม: เลดีกากา ซึ่งชนะรางวัลแกรมมี ห...</td>\n",
              "      <td>1. เลดีกากา ชนะแกรมมีกี่รางวัล A: หก 2. เลดีกา...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>สร้าง 15 คำถาม: ขณะที่เหลือเวลาอีก 4:51 นาทีแค...</td>\n",
              "      <td>1. แคโรไลนาเริ่มเล่นที่เส้นหลาที่เท่าไรเมื่อมี...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4485</th>\n",
              "      <td>สร้าง 2 คำถาม: ปกเกล้า อนันต์ สิบตำรวจตรี ปกเก...</td>\n",
              "      <td>1. ในปี 2560 ปกเกล้า อนันต์ เล่นในตำแหน่งกองกล...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4486</th>\n",
              "      <td>สร้าง 2 คำถาม: ธนาคารกสิกรไทย ธนาคารกสิกรไทย จ...</td>\n",
              "      <td>1. ธนาคารกสิกรไทยเป็นธนาคารในประเทศไทย มีสำนัก...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4487</th>\n",
              "      <td>สร้าง 2 คำถาม: ดิเรกสิน รัตนสิน พันโท ดิเรกสิน...</td>\n",
              "      <td>1. พันโท ดิเรกสิน รัตนสิน เกิดเมื่อวันที่เท่าไ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4488</th>\n",
              "      <td>สร้าง 2 คำถาม: วรพล ทองคำชู วรพล ทองคำชู นักเท...</td>\n",
              "      <td>1. นักเทนนิสชายไทย วรพล ทองคำชู ได้เหรียญทองจา...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4489</th>\n",
              "      <td>สร้าง 2 คำถาม: แอนนา ฟาริส แอนนา เคย์ ฟาริส (;...</td>\n",
              "      <td>1. ในปี 2008 แอนนา เคย์ ฟาริสได้แสดงภาพยนตร์เร...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4490 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e09f753-e30b-4d76-b82f-d63e51da632e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e09f753-e30b-4d76-b82f-d63e51da632e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e09f753-e30b-4d76-b82f-d63e51da632e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            source_text  \\\n",
              "0     สร้าง 14 คำถาม: ﻿ทีมรับของแพนเธอร์สถอดใจที่คะแ...   \n",
              "1     สร้าง 16 คำถาม: ทีมบรอนคอส เอาชนะทีม พิตต์สเบิ...   \n",
              "2     สร้าง 17 คำถาม: เพย์ตัน แมนนิง กลายเป็นควอเตอร...   \n",
              "3     สร้าง 12 คำถาม: เลดีกากา ซึ่งชนะรางวัลแกรมมี ห...   \n",
              "4     สร้าง 15 คำถาม: ขณะที่เหลือเวลาอีก 4:51 นาทีแค...   \n",
              "...                                                 ...   \n",
              "4485  สร้าง 2 คำถาม: ปกเกล้า อนันต์ สิบตำรวจตรี ปกเก...   \n",
              "4486  สร้าง 2 คำถาม: ธนาคารกสิกรไทย ธนาคารกสิกรไทย จ...   \n",
              "4487  สร้าง 2 คำถาม: ดิเรกสิน รัตนสิน พันโท ดิเรกสิน...   \n",
              "4488  สร้าง 2 คำถาม: วรพล ทองคำชู วรพล ทองคำชู นักเท...   \n",
              "4489  สร้าง 2 คำถาม: แอนนา ฟาริส แอนนา เคย์ ฟาริส (;...   \n",
              "\n",
              "                                            target_text  \n",
              "0     1. ทีมรับของแพนเธอร์สยอมแพ้ที่คะแนนเท่าไร A: 3...  \n",
              "1     1. ใครพ่ายแพ้ให้แก่ทีมบรอนคอสในรอบดิวิชั่น A: ...  \n",
              "2     1. เพย์ตัน แมนนิง อายุเท่าไรตอนที่เขาเล่นในซูเ...  \n",
              "3     1. เลดีกากา ชนะแกรมมีกี่รางวัล A: หก 2. เลดีกา...  \n",
              "4     1. แคโรไลนาเริ่มเล่นที่เส้นหลาที่เท่าไรเมื่อมี...  \n",
              "...                                                 ...  \n",
              "4485  1. ในปี 2560 ปกเกล้า อนันต์ เล่นในตำแหน่งกองกล...  \n",
              "4486  1. ธนาคารกสิกรไทยเป็นธนาคารในประเทศไทย มีสำนัก...  \n",
              "4487  1. พันโท ดิเรกสิน รัตนสิน เกิดเมื่อวันที่เท่าไ...  \n",
              "4488  1. นักเทนนิสชายไทย วรพล ทองคำชู ได้เหรียญทองจา...  \n",
              "4489  1. ในปี 2008 แอนนา เคย์ ฟาริสได้แสดงภาพยนตร์เร...  \n",
              "\n",
              "[4490 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def download_dataset(url, file_name):\n",
        "    urllib.request.urlretrieve(\n",
        "        url,\n",
        "        os.path.join(\"dataset/\", file_name),\n",
        "        reporthook=(\n",
        "            lambda count, block, total: print(\n",
        "                f\"Downloading {file_name}: {math.floor((count * block) / total * 100)}%\",\n",
        "                end=\"\\r\",\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "    print(f\"Downloaded {file_name} from {url}\")\n",
        "\n",
        "\n",
        "# Check if the dataset already exists\n",
        "if not (\n",
        "    os.path.exists(\"dataset/xquad.json\")\n",
        "    and os.path.exists(\"dataset/iapp-thai-wikipedia-qa.json\")\n",
        "):\n",
        "    os.mkdir(\"dataset\")\n",
        "    # Download all datasets\n",
        "    download_dataset(\n",
        "        \"https://github.com/deepmind/xquad/raw/master/xquad.th.json\", \"xquad.json\"\n",
        "    )\n",
        "    download_dataset(\n",
        "        \"https://raw.githubusercontent.com/iapp-technology/iapp-wiki-qa-dataset/main/iapp-thai-wikipedia-qa-1961-docs-9170-questions.json\",\n",
        "        \"iapp-thai-wikipedia-qa.json\",\n",
        "    )\n",
        "    download_dataset(\n",
        "        \"https://github.com/PyThaiNLP/thaiqa_squad/raw/main/data.zip\", \"thaiqa.zip\"\n",
        "    )\n",
        "    with ZipFile(\"dataset/thaiqa.zip\") as zipfile:\n",
        "        os.mkdir(\"dataset/thaiqa\")\n",
        "        zipfile.extractall(\"dataset/thaiqa/\")\n",
        "\n",
        "# This list will store all the Q&A\n",
        "source_list = []\n",
        "target_list = []\n",
        "\n",
        "# Start cleaning data\n",
        "squad = open(os.path.join(\"dataset/\", \"xquad.json\"))\n",
        "iapp = open(os.path.join(\"dataset/\", \"iapp-thai-wikipedia-qa.json\"))\n",
        "iapp_keys = open(os.path.join(\"dataset/\", \"iapp-thai-wikipedia-qa.json\"))\n",
        "thaiqa = open(os.path.join(\"dataset/thaiqa/data/train.jsonl\"))\n",
        "\n",
        "squad_json = ijson.items(squad, \"data.item\")\n",
        "iapp_json = json.load(iapp)\n",
        "iapp_keys = ijson.kvitems(iapp_keys, \"db\")\n",
        "thaiqa_df = pd.read_json(thaiqa, lines=True)\n",
        "\n",
        "# Get data from xquad\n",
        "for obj in squad_json:\n",
        "    paragraphs = obj[\"paragraphs\"]\n",
        "    for p in paragraphs:\n",
        "        context = p[\"context\"]\n",
        "        qas = [p for p in p[\"qas\"] if len(p) > 0]\n",
        "\n",
        "        source_text = f\"สร้าง {len(qas)} คำถาม: {context}\"\n",
        "        target_text = \"\"\n",
        "\n",
        "        for number, qa in enumerate(qas):\n",
        "            target_text += (\n",
        "                f\"{number + 1}. {qa['question']} A: {qa['answers'][0]['text']} \"\n",
        "            )\n",
        "\n",
        "        source_list.append(source_text.strip())\n",
        "        target_list.append(target_text.strip())\n",
        "\n",
        "# Get dataset from iapp\n",
        "for key in iapp_keys:\n",
        "    try:\n",
        "        obj = iapp_json[\"db\"][key[0]]\n",
        "        context = obj[\"detail\"]\n",
        "        qas = obj[\"QA\"]\n",
        "        target_text = \"\"\n",
        "\n",
        "        qa_amount = 0\n",
        "\n",
        "        for number, qa in enumerate(qas):\n",
        "            if len(qa[\"a\"]) != 0 and len(qa[\"q\"]) != 0:\n",
        "                target_text += f\"{number + 1}. {qa['q']} A: {qa['a'][0]} \"\n",
        "                qa_amount += 1\n",
        "\n",
        "        source_text = f\"สร้าง {qa_amount} คำถาม: {context}\"\n",
        "        source_list.append(source_text.strip())\n",
        "        target_list.append(target_text.strip())\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Due to the dataset, there will always be a keyerror on \"detail\" which is the dataset's metadata\n",
        "        if str(e) != \"'detail'\":\n",
        "            print(f\"KeyError: {e}\")\n",
        "\n",
        "# Get data from thaiqa\n",
        "article_ids = set(thaiqa_df[\"article_id\"])\n",
        "for id in article_ids:\n",
        "    questions = thaiqa_df[thaiqa_df[\"article_id\"] == id]\n",
        "\n",
        "    # Remove html markup\n",
        "    soup = BeautifulSoup(questions[\"context\"].iloc[0])\n",
        "\n",
        "    # Remove parenthesis because some are empty\n",
        "    context = re.sub(r\"\\(\\)\", \"\", soup.text)\n",
        "\n",
        "    # Remove double spaces resulting from removing parenthesis\n",
        "    context = re.sub(r\"\\s\\s+\", \" \", context)\n",
        "\n",
        "    source_text = f\"สร้าง {len(questions)} คำถาม: {context}\"\n",
        "    target_text = \"\"\n",
        "\n",
        "    qa_number = 1\n",
        "    for _, question in questions.iterrows():\n",
        "        target_text += f\"{qa_number}. {question['question']} A: {question['answer']} \"\n",
        "        qa_number += 1\n",
        "\n",
        "    source_list.append(source_text.strip())\n",
        "    target_list.append(target_text.strip())\n",
        "\n",
        "dataframe = pd.DataFrame({\"source_text\": source_list, \"target_text\": target_list})\n",
        "dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9omjA6Hngr"
      },
      "source": [
        "And then split our dataframe into train, valid, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-9XrtGfkHs7e"
      },
      "outputs": [],
      "source": [
        "index_train_split = math.floor(dataframe.shape[0] * 0.8)\n",
        "train_df, valid_test = (\n",
        "    dataframe.iloc[\n",
        "        :index_train_split,\n",
        "    ],\n",
        "    dataframe.iloc[\n",
        "        index_train_split:,\n",
        "    ],\n",
        ")\n",
        "\n",
        "index_test_split = math.floor(valid_test.shape[0] * 0.5)\n",
        "valid_df, test_df = (\n",
        "    valid_test.iloc[\n",
        "        :index_test_split,\n",
        "    ],\n",
        "    valid_test.iloc[\n",
        "        index_test_split:,\n",
        "    ],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VbYBMzdAHry"
      },
      "source": [
        "# Training\n",
        "Now we can finally get to the nitty-gritty and start defining our training loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atE8E0FNAHry",
        "outputId": "f7de1b1a-c1c6-49ce-f039-7fc344dba169"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 16\n"
          ]
        }
      ],
      "source": [
        "pl.seed_everything(16)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class MT5Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.data = df.reset_index()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_max_len = 1024\n",
        "        self.target_max_len = 1024\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_row = self.data.iloc[idx]\n",
        "        source, target = data_row[\"source_text\"], data_row[\"target_text\"]\n",
        "\n",
        "        source_encoding = self.tokenizer(\n",
        "            source,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            target,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Ensure labels are correct (see huggingface T5 training documentation)\n",
        "        labels = target_encoding.input_ids\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return dict(\n",
        "            input_ids=source_encoding.input_ids.flatten(),\n",
        "            attention_mask=source_encoding.attention_mask.flatten(),\n",
        "            decoder_input_ids=labels.flatten(),\n",
        "            decoder_attention_mask=target_encoding.attention_mask.flatten(),\n",
        "        )\n",
        "\n",
        "\n",
        "class MT5DataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        train_df,\n",
        "        valid_df,\n",
        "        test_df,\n",
        "        batch_size: int = 1,\n",
        "        num_workers: int = 2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.test_df = test_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None, batch_size=1):\n",
        "        self.batch_size = batch_size\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_data = MT5Dataset(self.train_df, self.tokenizer)\n",
        "            self.valid_data = MT5Dataset(self.valid_df, self.tokenizer)\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_data = MT5Dataset(self.test_df, self.tokenizer)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data, batch_size=self.batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.valid_data, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "\n",
        "class MT5Lightning(pl.LightningModule):\n",
        "    def __init__(self, model, tokenizer):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.avg_training_loss = None\n",
        "        self.avg_val_loss = None\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask\n",
        "    ):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        output = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        self.log(\"loss\", output[0], prog_bar=True, on_step=True, on_epoch=True)\n",
        "\n",
        "        return output[0]\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        output = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        self.log(\"val_loss\", output[0], prog_bar=True, on_step=True, on_epoch=True)\n",
        "\n",
        "        return output[0]\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        output = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        self.log(\"test_loss\", output.loss, prog_bar=True)\n",
        "\n",
        "        return output.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=3e-4)\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        self.avg_training_loss = np.round(\n",
        "            torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])).item(),\n",
        "            4,\n",
        "        )\n",
        "        path = \"\"\n",
        "        if os.path.exists(\"drive\"):\n",
        "            path += \"drive/MyDrive/mt5-thai-qg/\"\n",
        "        else:\n",
        "            path += \"outputs/\"\n",
        "        path += f\"mt5-qg-epoch-{self.current_epoch}-train-loss-{self.avg_training_loss}-val-loss-{self.avg_val_loss}\"\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "        self.model.save_pretrained(path)\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        _loss = [x.cpu() for x in validation_step_outputs]\n",
        "        self.avg_val_loss = np.round(\n",
        "            torch.mean(torch.stack(_loss)).item(),\n",
        "            4,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47el2aZzWinO"
      },
      "source": [
        "## Actually Train\n",
        "Start actually training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8JCn9TOXBkZ",
        "outputId": "89d77ceb-0e56-4eac-e6c0-a3059c12f428"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "model = MT5ForConditionalGeneration.from_pretrained(\n",
        "    \"google/mt5-small\", return_dict=True\n",
        ")\n",
        "tokenizer = MT5TokenizerFast.from_pretrained(\"google/mt5-small\")\n",
        "dataset = MT5DataModule(tokenizer, train_df, valid_df, test_df, batch_size=1)\n",
        "\n",
        "MT5Model = MT5Lightning(model, tokenizer)\n",
        "\n",
        "callbacks = []\n",
        "callbacks.append(EarlyStopping(monitor=\"val_loss\", mode=\"min\"))\n",
        "# callbacks.append(ORTCallback())\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    logger=True,\n",
        "    max_epochs=20,\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=callbacks,\n",
        "    profiler=\"simple\",\n",
        ")\n",
        "trainer.fit(MT5Model, dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYHPsomEAHr1"
      },
      "source": [
        "# Testing & Inferencing\n",
        "Now we will load up our model and do some inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0LBea3SAHr2",
        "outputId": "8481cbe3-aa04-4512-a0cb-e00c55bdcd75"
      },
      "outputs": [],
      "source": [
        "model = MT5ForConditionalGeneration.from_pretrained(\n",
        "    \"drive/MyDrive/mt5-thai-qg/mt5-qg-epoch-4-train-loss-0.961-val-loss-0.9701\",\n",
        "    return_dict=True,\n",
        ")\n",
        "tokenizer = MT5TokenizerFast.from_pretrained(\n",
        "    \"drive/MyDrive/mt5-thai-qg/mt5-qg-epoch-4-train-loss-0.961-val-loss-0.9701\"\n",
        ")\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "def predict(text):\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "        input_ids = input_ids.cuda()\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            num_beams=2,\n",
        "            max_length=1024,\n",
        "            repetition_penalty=1.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            top_p=50,\n",
        "            top_k=0.95,\n",
        "            num_return_sequences=1,\n",
        "        )\n",
        "\n",
        "        preds = [\n",
        "            tokenizer.decode(\n",
        "                g,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "            for g in generated_ids\n",
        "        ]\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this cell below to try out the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_to_predict = \"\"\"สร้าง 3 คำถาม: ดันเจียนซีจ (อังกฤษ: Dungeon Siege) เป็นเกมแอ็กชันเล่นตามบทบาทที่พัฒนาโดยแก๊สเพาเวิร์ดเกมส์ ซึ่งไมโครซอฟท์ได้จัดจำหน่ายบนแพลตฟอร์มไมโครซอฟท์ วินโดวส์ ในเดือนเมษายน ค.ศ. 2002 และเดสทิเนียร์ได้จัดจำหน่ายบนแพลตฟอร์มแมคโอเอสเท็นในปีถัดไป โดยมีฉากอยู่ในอาณาจักรยุคกลางสมมติ ชื่อ เอห์บ เกมนี้ยังจัดเป็นแนวแฟนตาซีระดับสูงที่เดินเรื่องตามชาวไร่หนุ่มคนหนึ่งและเพื่อนร่วมทางขณะที่พวกเขาออกเดินทางเพื่อกำจัดกองกำลังที่รุกราน ในตอนแรก กลุ่มตัวเอกเพียงต้องการเตือนเมืองใกล้เคียงเกี่ยวกับการรุกรานของเผ่าพันธุ์สิ่งมีชีวิตที่ชื่อครุก และในอีกไม่นาน ชาวไร่คนดังกล่าวและเพื่อนร่วมทางกับเขาตกอยู่ในสถานการณ์หาทางเอาชนะเผ่าพันธุ์อื่นที่เรียกว่าเซกอย่างหลีกเลี่ยงไม่ได้ ซึ่งฟื้นคืนพลังใหม่หลังจากถูกคุมขังอยู่ 300 ปี โลกของดันเจียนซีจไม่ใช้ระบบเลเวลเหมือนกับวิดีโอเกมเล่นตามบทบาทอื่น ๆ ในยุคนั้น หากแต่เป็นพื้นที่เดียวที่ต่อเนื่อง โดยปราศจากการโหลดหน้าจอ ซึ่งผู้เล่นเดินทางผ่านเพื่อต่อสู้กับฝูงศัตรู นอกจากนี้ แทนที่จะกำหนดคลาสตัวละครและควบคุมตัวละครทั้งหมดในกลุ่มด้วยตนเอง ผู้เล่นจะควบคุมกลยุทธ์และอาวุธ ตลอดจนการใช้เวทมนตร์โดยรวมของพวกเขา ซึ่งกำกับการเติบโตของตัวละคร\"\"\"\n",
        "print(predict(text_to_predict))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup our dataset for testm mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ch5VFBFEjFHC"
      },
      "outputs": [],
      "source": [
        "dataset.setup(stage=\"test\", batch_size=4)\n",
        "test_loader = dataset.test_dataloader()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define a preprocessing function so that our metrics don't die"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4nYE23SrxY1Z"
      },
      "outputs": [],
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "\n",
        "def pre_process(texts):\n",
        "    final = []\n",
        "    for text in texts:\n",
        "        final.append(\" \".join(word_tokenize(text, keep_whitespace=False)))\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our evaluation will use the following metrics:\n",
        "* ROUGE\n",
        "* CHRF\n",
        "* GLEU\n",
        "* METEOR\n",
        "\n",
        "Begin the evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W46w_hzMekps",
        "outputId": "84cc7dab-be04-4661-9cd6-85283c180b8b"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics import CHRFScore, BLEUScore\n",
        "from nltk.translate import meteor_score, gleu_score\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "bleu = BLEUScore(n_gram=4, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "rouge = ROUGEScore()\n",
        "chrf = CHRFScore()\n",
        "\n",
        "meteor_avg = 0\n",
        "gleu_avg = 0\n",
        "\n",
        "labels_collect = []\n",
        "preds_collect = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"].cuda(),\n",
        "        num_beams=2,\n",
        "        max_length=1024,\n",
        "        repetition_penalty=1.5,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True,\n",
        "        top_p=50,\n",
        "        top_k=0.95,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "    preds = [\n",
        "        tokenizer.decode(\n",
        "            g,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "        for g in generated_ids\n",
        "    ]\n",
        "\n",
        "    batch[\"decoder_input_ids\"][\n",
        "        batch[\"decoder_input_ids\"] == -100\n",
        "    ] = tokenizer.pad_token_id\n",
        "\n",
        "    labels = [\n",
        "        tokenizer.decode(\n",
        "            g,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "        for g in batch[\"decoder_input_ids\"]\n",
        "    ]\n",
        "\n",
        "    preds = pre_process(preds)\n",
        "    labels = pre_process(labels)\n",
        "\n",
        "    for p, l in zip(preds, labels):\n",
        "        meteor_avg += meteor_score.single_meteor_score(p.split(\" \"), l.split(\" \"))\n",
        "        gleu_avg += gleu_score.sentence_gleu(p.split(\" \"), l.split(\" \"))\n",
        "\n",
        "    bleu(preds, labels)\n",
        "    chrf(preds, labels)\n",
        "    rouge(preds, labels)\n",
        "    preds_collect.append(preds)\n",
        "    labels_collect.append(labels)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Meteor: {meteor_avg / (4 * 113)}\")\n",
        "print(f\"GLEU: {gleu_avg / (4 * 113)}\")\n",
        "print(f\"BLEU: {bleu.compute().item()}\")\n",
        "print(f\"CHRF: {chrf.compute().item()}\")\n",
        "print(f\"ROUGE: {rouge.compute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Misc\n",
        "Below are some other useful chunks of code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export prediction & labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iOPRt-v-tad",
        "outputId": "638b83ba-3387-4128-d6e6-2ce22ab7fc09"
      },
      "outputs": [],
      "source": [
        "print(labels_collect[0][1])\n",
        "print(preds_collect[0][1])\n",
        "\n",
        "label_final = [i for x in labels_collect for i in x]\n",
        "pred_final = [i for x in preds_collect for i in x]\n",
        "\n",
        "\n",
        "export = pd.DataFrame(\n",
        "    data=zip(label_final, pred_final), columns=[\"Labels\", \"Predictions\"]\n",
        ")\n",
        "export.to_json(\"output.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zip up Outputs\n",
        "Zip up the outputs & logs folder (with the models) to prepare them for exporting using zstd.\n",
        "\n",
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!apt install tar zstd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zipping up everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tar -c -I \"zstd -19 -T0\" -f \"mt5-thai-qg.tar.zst\" outputs/ lightning_logs/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mT5_train_thai (7).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "599dbc988367df75f394341c14191e4f45ca195597a9b0aa004fda5844c2c20f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
