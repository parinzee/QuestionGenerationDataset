{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbwjJF8nAHrn"
      },
      "source": [
        "# Welcome to mt5-thai-QG\n",
        "This is notebook detailing how to finetune **mt5 for question-generation in the Thai language** \n",
        "\n",
        "\n",
        "First, we will mount our google drive so our models don't get deleted (╯°□°)╯︵ ┻━┻ \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ecgcLxQBIym",
        "outputId": "95a5053f-1117-4abe-9df1-caf1b35cd1fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8Pj_wDYBKS4"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Now we will install some requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oy4QtOY3AHrq",
        "outputId": "4a3d3ea7-aa74-4a00-fcec-9afc9f3f976d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ijson\n",
            "  Downloading ijson-3.1.4-cp37-cp37m-manylinux2010_x86_64.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 60.6 MB/s \n",
            "\u001b[?25hCollecting lightning-bolts\n",
            "  Downloading lightning_bolts-0.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 62.1 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 33.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 30.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (3.17.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 61.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Collecting PyYAML>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.3 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.64.0)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.46.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 64.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.4.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 68.8 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, tokenizers, pytorch-lightning, huggingface-hub, transformers, sentencepiece, lightning-bolts, ijson\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.5.0 huggingface-hub-0.7.0 ijson-3.1.4 lightning-bolts-0.5.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 sentencepiece-0.1.96 tokenizers-0.12.1 torchmetrics-0.9.1 transformers-4.19.4 yarl-1.7.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pythainlp\n",
            "  Downloading pythainlp-3.0.8-py3-none-any.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 6.6 MB/s \n",
            "\u001b[?25hCollecting epitran\n",
            "  Downloading epitran-1.22-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.7/dist-packages (from pythainlp) (2.23.0)\n",
            "Collecting tinydb>=3.0\n",
            "  Downloading tinydb-4.7.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.22.0->pythainlp) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=3.10.0 in /usr/local/lib/python3.7/dist-packages (from tinydb>=3.0->pythainlp) (4.2.0)\n",
            "Collecting marisa-trie\n",
            "  Downloading marisa_trie-0.7.7-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from epitran) (57.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from epitran) (2019.12.20)\n",
            "Collecting unicodecsv\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "Collecting panphon>=0.19\n",
            "  Downloading panphon-0.20.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[K     |████████████████████████████████| 73 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (1.21.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from panphon>=0.19->epitran) (6.0)\n",
            "Collecting munkres\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10768 sha256=e57235b4ab75e7b40c32196fab2d0cd09dccdebb55349f347db1684dea6e80a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/f4/8a/a5024fb77b32ed369e5c409081e5f00fbe3b92fdad653f6e69\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, munkres, tinydb, panphon, marisa-trie, pythainlp, epitran\n",
            "Successfully installed epitran-1.22 marisa-trie-0.7.7 munkres-1.1.4 panphon-0.20.0 pythainlp-3.0.8 tinydb-4.7.0 unicodecsv-0.14.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Collecting regex>=2021.8.3\n",
            "  Downloading regex-2022.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Installing collected packages: regex, nltk\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.7 regex-2022.6.2\n"
          ]
        }
      ],
      "source": [
        "# Remove \"sample_data\" in colab\n",
        "!rm -rf sample_data\n",
        "# Solve some protobuf problems\n",
        "!export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION='python'\n",
        "# Install programs\n",
        "\n",
        "## Install ONNX\n",
        "#%pip install torch-ort onnxruntime-training -f https://download.onnxruntime.ai/onnxruntime_stable_cu111.html\n",
        "#!apt install ninja-build\n",
        "#!python -m torch_ort.configure\n",
        "\n",
        "## Install other stuff\n",
        "%pip install ijson pandas torchmetrics lightning-bolts transformers sentencepiece protobuf beautifulsoup4 pytorch-lightning \n",
        "%pip install pythainlp epitran\n",
        "%pip install -U nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXaB7UrtCyNz"
      },
      "source": [
        "Then we will import all the things we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "thZlTjPEC2Hl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import urllib.request\n",
        "import os\n",
        "import ijson\n",
        "import json\n",
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "# from pl_bolts.callbacks import ORTCallback\n",
        "\n",
        "from zipfile import ZipFile\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    MT5TokenizerFast,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1xgys6NAHrt"
      },
      "source": [
        "# Gather & Process datasets\n",
        "- xquad-thai\n",
        "- iapp-wiki-qa-dataset\n",
        "- thaiqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "CkNR3MnNAHrt",
        "outputId": "7326174c-a1e9-49cc-c5f8-3fd0793f6709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded xquad.json from https://github.com/deepmind/xquad/raw/master/xquad.th.json\n",
            "Downloaded iapp-thai-wikipedia-qa.json from https://raw.githubusercontent.com/iapp-technology/iapp-wiki-qa-dataset/main/iapp-thai-wikipedia-qa-1961-docs-9170-questions.json\n",
            "Downloaded thaiqa.zip from https://github.com/PyThaiNLP/thaiqa_squad/raw/main/data.zip\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3dfbedfe-dc53-42ef-a2da-b9aab1ff7c48\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source_text</th>\n",
              "      <th>target_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>สร้าง 14 คำถาม: ﻿ทีมรับของแพนเธอร์สถอดใจที่คะแ...</td>\n",
              "      <td>1. ทีมรับของแพนเธอร์สยอมแพ้ที่คะแนนเท่าไร A: 3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>สร้าง 16 คำถาม: ทีมบรอนคอส เอาชนะทีม พิตต์สเบิ...</td>\n",
              "      <td>1. ใครพ่ายแพ้ให้แก่ทีมบรอนคอสในรอบดิวิชั่น A: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>สร้าง 17 คำถาม: เพย์ตัน แมนนิง กลายเป็นควอเตอร...</td>\n",
              "      <td>1. เพย์ตัน แมนนิง อายุเท่าไรตอนที่เขาเล่นในซูเ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>สร้าง 12 คำถาม: เลดีกากา ซึ่งชนะรางวัลแกรมมี ห...</td>\n",
              "      <td>1. เลดีกากา ชนะแกรมมีกี่รางวัล A: หก 2. เลดีกา...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>สร้าง 15 คำถาม: ขณะที่เหลือเวลาอีก 4:51 นาทีแค...</td>\n",
              "      <td>1. แคโรไลนาเริ่มเล่นที่เส้นหลาที่เท่าไรเมื่อมี...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4485</th>\n",
              "      <td>สร้าง 2 คำถาม: ปกเกล้า อนันต์ สิบตำรวจตรี ปกเก...</td>\n",
              "      <td>1. ในปี 2560 ปกเกล้า อนันต์ เล่นในตำแหน่งกองกล...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4486</th>\n",
              "      <td>สร้าง 2 คำถาม: ธนาคารกสิกรไทย ธนาคารกสิกรไทย จ...</td>\n",
              "      <td>1. ธนาคารกสิกรไทยเป็นธนาคารในประเทศไทย มีสำนัก...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4487</th>\n",
              "      <td>สร้าง 2 คำถาม: ดิเรกสิน รัตนสิน พันโท ดิเรกสิน...</td>\n",
              "      <td>1. พันโท ดิเรกสิน รัตนสิน เกิดเมื่อวันที่เท่าไ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4488</th>\n",
              "      <td>สร้าง 2 คำถาม: วรพล ทองคำชู วรพล ทองคำชู นักเท...</td>\n",
              "      <td>1. นักเทนนิสชายไทย วรพล ทองคำชู ได้เหรียญทองจา...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4489</th>\n",
              "      <td>สร้าง 2 คำถาม: แอนนา ฟาริส แอนนา เคย์ ฟาริส (;...</td>\n",
              "      <td>1. ในปี 2008 แอนนา เคย์ ฟาริสได้แสดงภาพยนตร์เร...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4490 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dfbedfe-dc53-42ef-a2da-b9aab1ff7c48')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dfbedfe-dc53-42ef-a2da-b9aab1ff7c48 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dfbedfe-dc53-42ef-a2da-b9aab1ff7c48');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                            source_text  \\\n",
              "0     สร้าง 14 คำถาม: ﻿ทีมรับของแพนเธอร์สถอดใจที่คะแ...   \n",
              "1     สร้าง 16 คำถาม: ทีมบรอนคอส เอาชนะทีม พิตต์สเบิ...   \n",
              "2     สร้าง 17 คำถาม: เพย์ตัน แมนนิง กลายเป็นควอเตอร...   \n",
              "3     สร้าง 12 คำถาม: เลดีกากา ซึ่งชนะรางวัลแกรมมี ห...   \n",
              "4     สร้าง 15 คำถาม: ขณะที่เหลือเวลาอีก 4:51 นาทีแค...   \n",
              "...                                                 ...   \n",
              "4485  สร้าง 2 คำถาม: ปกเกล้า อนันต์ สิบตำรวจตรี ปกเก...   \n",
              "4486  สร้าง 2 คำถาม: ธนาคารกสิกรไทย ธนาคารกสิกรไทย จ...   \n",
              "4487  สร้าง 2 คำถาม: ดิเรกสิน รัตนสิน พันโท ดิเรกสิน...   \n",
              "4488  สร้าง 2 คำถาม: วรพล ทองคำชู วรพล ทองคำชู นักเท...   \n",
              "4489  สร้าง 2 คำถาม: แอนนา ฟาริส แอนนา เคย์ ฟาริส (;...   \n",
              "\n",
              "                                            target_text  \n",
              "0     1. ทีมรับของแพนเธอร์สยอมแพ้ที่คะแนนเท่าไร A: 3...  \n",
              "1     1. ใครพ่ายแพ้ให้แก่ทีมบรอนคอสในรอบดิวิชั่น A: ...  \n",
              "2     1. เพย์ตัน แมนนิง อายุเท่าไรตอนที่เขาเล่นในซูเ...  \n",
              "3     1. เลดีกากา ชนะแกรมมีกี่รางวัล A: หก 2. เลดีกา...  \n",
              "4     1. แคโรไลนาเริ่มเล่นที่เส้นหลาที่เท่าไรเมื่อมี...  \n",
              "...                                                 ...  \n",
              "4485  1. ในปี 2560 ปกเกล้า อนันต์ เล่นในตำแหน่งกองกล...  \n",
              "4486  1. ธนาคารกสิกรไทยเป็นธนาคารในประเทศไทย มีสำนัก...  \n",
              "4487  1. พันโท ดิเรกสิน รัตนสิน เกิดเมื่อวันที่เท่าไ...  \n",
              "4488  1. นักเทนนิสชายไทย วรพล ทองคำชู ได้เหรียญทองจา...  \n",
              "4489  1. ในปี 2008 แอนนา เคย์ ฟาริสได้แสดงภาพยนตร์เร...  \n",
              "\n",
              "[4490 rows x 2 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def download_dataset(url, file_name):\n",
        "    urllib.request.urlretrieve(\n",
        "        url,\n",
        "        os.path.join(\"dataset/\", file_name),\n",
        "        reporthook=(\n",
        "            lambda count, block, total: print(\n",
        "                f\"Downloading {file_name}: {math.floor((count * block) / total * 100)}%\",\n",
        "                end=\"\\r\",\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "    print(f\"Downloaded {file_name} from {url}\")\n",
        "\n",
        "\n",
        "# Check if the dataset already exists\n",
        "if not (\n",
        "    os.path.exists(\"dataset/xquad.json\")\n",
        "    and os.path.exists(\"dataset/iapp-thai-wikipedia-qa.json\")\n",
        "):\n",
        "    os.mkdir(\"dataset\")\n",
        "    # Download all datasets\n",
        "    download_dataset(\n",
        "        \"https://github.com/deepmind/xquad/raw/master/xquad.th.json\", \"xquad.json\"\n",
        "    )\n",
        "    download_dataset(\n",
        "        \"https://raw.githubusercontent.com/iapp-technology/iapp-wiki-qa-dataset/main/iapp-thai-wikipedia-qa-1961-docs-9170-questions.json\",\n",
        "        \"iapp-thai-wikipedia-qa.json\",\n",
        "    )\n",
        "    download_dataset(\n",
        "        \"https://github.com/PyThaiNLP/thaiqa_squad/raw/main/data.zip\", \"thaiqa.zip\"\n",
        "    )\n",
        "    with ZipFile(\"dataset/thaiqa.zip\") as zipfile:\n",
        "        os.mkdir(\"dataset/thaiqa\")\n",
        "        zipfile.extractall(\"dataset/thaiqa/\")\n",
        "\n",
        "# This list will store all the Q&A\n",
        "source_list = []\n",
        "target_list = []\n",
        "\n",
        "# Start cleaning data\n",
        "squad = open(os.path.join(\"dataset/\", \"xquad.json\"))\n",
        "iapp = open(os.path.join(\"dataset/\", \"iapp-thai-wikipedia-qa.json\"))\n",
        "iapp_keys = open(os.path.join(\"dataset/\", \"iapp-thai-wikipedia-qa.json\"))\n",
        "thaiqa = open(os.path.join(\"dataset/thaiqa/data/train.jsonl\"))\n",
        "\n",
        "squad_json = ijson.items(squad, \"data.item\")\n",
        "iapp_json = json.load(iapp)\n",
        "iapp_keys = ijson.kvitems(iapp_keys, \"db\")\n",
        "thaiqa_df = pd.read_json(thaiqa, lines=True)\n",
        "\n",
        "# Get data from xquad\n",
        "for obj in squad_json:\n",
        "    paragraphs = obj[\"paragraphs\"]\n",
        "    for p in paragraphs:\n",
        "        context = p[\"context\"]\n",
        "        qas = [p for p in p[\"qas\"] if len(p) > 0]\n",
        "\n",
        "        source_text = f\"สร้าง {len(qas)} คำถาม: {context}\"\n",
        "        target_text = \"\"\n",
        "\n",
        "        for number, qa in enumerate(qas):\n",
        "            target_text += (\n",
        "                f\"{number + 1}. {qa['question']} A: {qa['answers'][0]['text']} \"\n",
        "            )\n",
        "\n",
        "        source_list.append(source_text.strip())\n",
        "        target_list.append(target_text.strip())\n",
        "\n",
        "# Get dataset from iapp\n",
        "for key in iapp_keys:\n",
        "    try:\n",
        "        obj = iapp_json[\"db\"][key[0]]\n",
        "        context = obj[\"detail\"]\n",
        "        qas = obj[\"QA\"]\n",
        "        target_text = \"\"\n",
        "\n",
        "        qa_amount = 0\n",
        "\n",
        "        for number, qa in enumerate(qas):\n",
        "            if len(qa[\"a\"]) != 0 and len(qa[\"q\"]) != 0:\n",
        "                target_text += f\"{number + 1}. {qa['q']} A: {qa['a'][0]} \"\n",
        "                qa_amount += 1\n",
        "\n",
        "        source_text = f\"สร้าง {qa_amount} คำถาม: {context}\"\n",
        "        source_list.append(source_text.strip())\n",
        "        target_list.append(target_text.strip())\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Due to the dataset, there will always be a keyerror on \"detail\" which is the dataset's metadata\n",
        "        if str(e) != \"'detail'\":\n",
        "            print(f\"KeyError: {e}\")\n",
        "\n",
        "# Get data from thaiqa\n",
        "article_ids = set(thaiqa_df[\"article_id\"])\n",
        "for id in article_ids:\n",
        "    questions = thaiqa_df[thaiqa_df[\"article_id\"] == id]\n",
        "\n",
        "    # Remove html markup\n",
        "    soup = BeautifulSoup(questions[\"context\"].iloc[0])\n",
        "\n",
        "    # Remove parenthesis because some are empty\n",
        "    context = re.sub(r\"\\(\\)\", \"\", soup.text)\n",
        "\n",
        "    # Remove double spaces resulting from removing parenthesis\n",
        "    context = re.sub(r\"\\s\\s+\", \" \", context)\n",
        "\n",
        "    source_text = f\"สร้าง {len(questions)} คำถาม: {context}\"\n",
        "    target_text = \"\"\n",
        "\n",
        "    qa_number = 1\n",
        "    for _, question in questions.iterrows():\n",
        "        target_text += f\"{qa_number}. {question['question']} A: {question['answer']} \"\n",
        "        qa_number += 1\n",
        "\n",
        "    source_list.append(source_text.strip())\n",
        "    target_list.append(target_text.strip())\n",
        "\n",
        "dataframe = pd.DataFrame({\"source_text\": source_list, \"target_text\": target_list})\n",
        "dataframe\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw9omjA6Hngr"
      },
      "source": [
        "And then split our dataframe into train, valid, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-9XrtGfkHs7e"
      },
      "outputs": [],
      "source": [
        "index_train_split = math.floor(dataframe.shape[0] * 0.8)\n",
        "train_df, valid_test = (\n",
        "    dataframe.iloc[\n",
        "        :index_train_split,\n",
        "    ],\n",
        "    dataframe.iloc[\n",
        "        index_train_split:,\n",
        "    ],\n",
        ")\n",
        "\n",
        "index_test_split = math.floor(valid_test.shape[0] * 0.5)\n",
        "valid_df, test_df = (\n",
        "    valid_test.iloc[\n",
        "        :index_test_split,\n",
        "    ],\n",
        "    valid_test.iloc[\n",
        "        index_test_split:,\n",
        "    ],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VbYBMzdAHry"
      },
      "source": [
        "# Training\n",
        "Now we can finally get to the nitty-gritty and start defining our training loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atE8E0FNAHry",
        "outputId": "c416d01c-1fc6-4fef-d5ca-05c1bb6f4a4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 16\n"
          ]
        }
      ],
      "source": [
        "pl.seed_everything(16)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "class MT5Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.data = df.reset_index()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.source_max_len = 1024\n",
        "        self.target_max_len = 1024\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_row = self.data.iloc[idx]\n",
        "        source, target = data_row[\"source_text\"], data_row[\"target_text\"]\n",
        "\n",
        "        source_encoding = self.tokenizer(\n",
        "            source,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        target_encoding = self.tokenizer(\n",
        "            target,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Ensure labels are correct (see huggingface T5 training documentation)\n",
        "        labels = target_encoding.input_ids\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return dict(\n",
        "            input_ids=source_encoding.input_ids.flatten(),\n",
        "            attention_mask=source_encoding.attention_mask.flatten(),\n",
        "            decoder_input_ids=labels.flatten(),\n",
        "            decoder_attention_mask=target_encoding.attention_mask.flatten(),\n",
        "        )\n",
        "\n",
        "\n",
        "class MT5DataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        train_df,\n",
        "        valid_df,\n",
        "        test_df,\n",
        "        batch_size: int = 1,\n",
        "        num_workers: int = 2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.test_df = test_df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def setup(self, stage: Optional[str] = None, batch_size=1):\n",
        "        self.batch_size = batch_size\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            self.train_data = MT5Dataset(self.train_df, self.tokenizer)\n",
        "            self.valid_data = MT5Dataset(self.valid_df, self.tokenizer)\n",
        "\n",
        "        if stage == \"test\" or stage is None:\n",
        "            self.test_data = MT5Dataset(self.test_df, self.tokenizer)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.train_data, batch_size=self.batch_size, shuffle=True\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.valid_data, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            self.test_data, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "\n",
        "class MT5Lightning(pl.LightningModule):\n",
        "    def __init__(self, model, tokenizer):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.avg_training_loss = None\n",
        "        self.avg_val_loss = None\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask\n",
        "    ):\n",
        "        output = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        output = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        self.log(\"loss\", output[0], prog_bar=True, on_step=True, on_epoch=True)\n",
        "\n",
        "        return output[0]\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        output = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        self.log(\"val_loss\", output[0], prog_bar=True, on_step=True, on_epoch=True)\n",
        "\n",
        "        return output[0]\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"]\n",
        "        decoder_attention_mask = batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        output = self(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "        )\n",
        "\n",
        "        self.log(\"test_loss\", output.loss, prog_bar=True)\n",
        "\n",
        "        return output.loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=3e-4)\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        self.avg_training_loss = np.round(\n",
        "            torch.mean(torch.stack([x[\"loss\"] for x in training_step_outputs])).item(),\n",
        "            4,\n",
        "        )\n",
        "        path = \"\"\n",
        "        if os.path.exists(\"drive\"):\n",
        "            path += \"drive/MyDrive/mt5-thai-qg/\"\n",
        "        else:\n",
        "            path += \"outputs/\"\n",
        "        path += f\"mt5-qg-epoch-{self.current_epoch}-train-loss-{self.avg_training_loss}-val-loss-{self.avg_val_loss}\"\n",
        "        self.tokenizer.save_pretrained(path)\n",
        "        self.model.save_pretrained(path)\n",
        "\n",
        "    def validation_epoch_end(self, validation_step_outputs):\n",
        "        _loss = [x.cpu() for x in validation_step_outputs]\n",
        "        self.avg_val_loss = np.round(\n",
        "            torch.mean(torch.stack(_loss)).item(),\n",
        "            4,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47el2aZzWinO"
      },
      "source": [
        "## Actually Train\n",
        "Start actually training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8JCn9TOXBkZ",
        "outputId": "89d77ceb-0e56-4eac-e6c0-a3059c12f428"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "model = MT5ForConditionalGeneration.from_pretrained(\n",
        "    \"google/mt5-small\", return_dict=True\n",
        ")\n",
        "tokenizer = MT5TokenizerFast.from_pretrained(\"google/mt5-small\")\n",
        "dataset = MT5DataModule(tokenizer, train_df, valid_df, test_df, batch_size=1)\n",
        "\n",
        "MT5Model = MT5Lightning(model, tokenizer)\n",
        "\n",
        "callbacks = []\n",
        "callbacks.append(EarlyStopping(monitor=\"val_loss\", mode=\"min\"))\n",
        "# callbacks.append(ORTCallback())\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    devices=1,\n",
        "    logger=True,\n",
        "    max_epochs=20,\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=callbacks,\n",
        "    profiler=\"simple\",\n",
        ")\n",
        "trainer.fit(MT5Model, dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYHPsomEAHr1"
      },
      "source": [
        "# Testing & Inferencing\n",
        "Now we will load up our model and do some inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00J8W8Hze71C"
      },
      "source": [
        "## Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q0LBea3SAHr2"
      },
      "outputs": [],
      "source": [
        "model = MT5ForConditionalGeneration.from_pretrained(\n",
        "    \"drive/MyDrive/mt5-thai-qg/mt5-qg-epoch-4-train-loss-0.961-val-loss-0.9701\",\n",
        "    return_dict=True,\n",
        ")\n",
        "tokenizer = MT5TokenizerFast.from_pretrained(\n",
        "    \"drive/MyDrive/mt5-thai-qg/mt5-qg-epoch-4-train-loss-0.961-val-loss-0.9701\"\n",
        ")\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "def predict(text):\n",
        "    with torch.no_grad():\n",
        "        input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "        input_ids = input_ids.cuda()\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            num_beams=2,\n",
        "            max_length=1024,\n",
        "            repetition_penalty=1.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            top_p=50,\n",
        "            top_k=0.95,\n",
        "            num_return_sequences=1,\n",
        "        )\n",
        "\n",
        "        preds = [\n",
        "            tokenizer.decode(\n",
        "                g,\n",
        "                skip_special_tokens=True,\n",
        "                clean_up_tokenization_spaces=True,\n",
        "            )\n",
        "            for g in generated_ids\n",
        "        ]\n",
        "    return preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs9gTmzde71D"
      },
      "source": [
        "Run this cell below to try out the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QD7ZlxCLe71E"
      },
      "outputs": [],
      "source": [
        "text_to_predict = \"\"\"สร้าง 3 คำถาม: ดันเจียนซีจ (อังกฤษ: Dungeon Siege) เป็นเกมแอ็กชันเล่นตามบทบาทที่พัฒนาโดยแก๊สเพาเวิร์ดเกมส์ ซึ่งไมโครซอฟท์ได้จัดจำหน่ายบนแพลตฟอร์มไมโครซอฟท์ วินโดวส์ ในเดือนเมษายน ค.ศ. 2002 และเดสทิเนียร์ได้จัดจำหน่ายบนแพลตฟอร์มแมคโอเอสเท็นในปีถัดไป โดยมีฉากอยู่ในอาณาจักรยุคกลางสมมติ ชื่อ เอห์บ เกมนี้ยังจัดเป็นแนวแฟนตาซีระดับสูงที่เดินเรื่องตามชาวไร่หนุ่มคนหนึ่งและเพื่อนร่วมทางขณะที่พวกเขาออกเดินทางเพื่อกำจัดกองกำลังที่รุกราน ในตอนแรก กลุ่มตัวเอกเพียงต้องการเตือนเมืองใกล้เคียงเกี่ยวกับการรุกรานของเผ่าพันธุ์สิ่งมีชีวิตที่ชื่อครุก และในอีกไม่นาน ชาวไร่คนดังกล่าวและเพื่อนร่วมทางกับเขาตกอยู่ในสถานการณ์หาทางเอาชนะเผ่าพันธุ์อื่นที่เรียกว่าเซกอย่างหลีกเลี่ยงไม่ได้ ซึ่งฟื้นคืนพลังใหม่หลังจากถูกคุมขังอยู่ 300 ปี โลกของดันเจียนซีจไม่ใช้ระบบเลเวลเหมือนกับวิดีโอเกมเล่นตามบทบาทอื่น ๆ ในยุคนั้น หากแต่เป็นพื้นที่เดียวที่ต่อเนื่อง โดยปราศจากการโหลดหน้าจอ ซึ่งผู้เล่นเดินทางผ่านเพื่อต่อสู้กับฝูงศัตรู นอกจากนี้ แทนที่จะกำหนดคลาสตัวละครและควบคุมตัวละครทั้งหมดในกลุ่มด้วยตนเอง ผู้เล่นจะควบคุมกลยุทธ์และอาวุธ ตลอดจนการใช้เวทมนตร์โดยรวมของพวกเขา ซึ่งกำกับการเติบโตของตัวละคร\"\"\"\n",
        "print(predict(text_to_predict))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYilwSaXe71F"
      },
      "source": [
        "## Evaluate Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ayStuz4e71F"
      },
      "source": [
        "Setup our dataset for testm mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ch5VFBFEjFHC"
      },
      "outputs": [],
      "source": [
        "dataset = MT5DataModule(tokenizer, train_df, valid_df, test_df, batch_size=4)\n",
        "dataset.setup(stage=\"test\")\n",
        "test_loader = dataset.test_dataloader()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcQobh-Ze71G"
      },
      "source": [
        "Define a preprocessing function so that our metrics don't die"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4nYE23SrxY1Z"
      },
      "outputs": [],
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "\n",
        "def pre_process(texts):\n",
        "    final = []\n",
        "    for text in texts:\n",
        "        final.append(\" \".join(word_tokenize(text, keep_whitespace=False)))\n",
        "    return final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wffoj8Ee71I"
      },
      "source": [
        "Our evaluation will use the following metrics:\n",
        "* ROUGE\n",
        "* CHRF\n",
        "* GLEU\n",
        "* METEOR\n",
        "\n",
        "Begin the evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W46w_hzMekps",
        "outputId": "c2ff94d0-72ad-4910-aead-94bfc612ee9f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.7/dist-packages/torchmetrics/functional/text/chrf.py:181: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  total_n_grams[n] = tensor(sum(n_grams_counts[n].values()))\n",
            "/usr/local/lib/python3.7/dist-packages/torchmetrics/functional/text/chrf.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  for n_gram in hyp_n_grams_counts[n]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------\n",
            "Meteor: 0.5409517550337343\n",
            "GLEU: 0.011632674913384072\n",
            "BLEU: 1.40121632714385e-157\n",
            "CHRF: 0.4429878294467926\n",
            "ROUGE: {'rouge1_fmeasure': tensor(0.8655), 'rouge1_precision': tensor(0.8883), 'rouge1_recall': tensor(0.8775), 'rouge2_fmeasure': tensor(0.7003), 'rouge2_precision': tensor(0.7207), 'rouge2_recall': tensor(0.7169), 'rougeL_fmeasure': tensor(0.8457), 'rougeL_precision': tensor(0.8684), 'rougeL_recall': tensor(0.8574), 'rougeLsum_fmeasure': tensor(0.8599), 'rougeLsum_precision': tensor(0.8825), 'rougeLsum_recall': tensor(0.8720)}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from torchmetrics.text.rouge import ROUGEScore\n",
        "from torchmetrics import CHRFScore\n",
        "from nltk.translate import meteor_score, gleu_score, bleu_score\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "rouge = ROUGEScore()\n",
        "chrf = CHRFScore()\n",
        "\n",
        "bleu_avg = 0\n",
        "meteor_avg = 0\n",
        "gleu_avg = 0\n",
        "\n",
        "labels_collect = []\n",
        "preds_collect = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    generated_ids = model.generate(\n",
        "        input_ids=batch[\"input_ids\"].cuda(),\n",
        "        num_beams=2,\n",
        "        max_length=1024,\n",
        "        repetition_penalty=1.5,\n",
        "        length_penalty=1.0,\n",
        "        early_stopping=True,\n",
        "        top_p=50,\n",
        "        top_k=0.95,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "\n",
        "    preds = [\n",
        "        tokenizer.decode(\n",
        "            g,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "        for g in generated_ids\n",
        "    ]\n",
        "\n",
        "    batch[\"decoder_input_ids\"][\n",
        "        batch[\"decoder_input_ids\"] == -100\n",
        "    ] = tokenizer.pad_token_id\n",
        "\n",
        "    labels = [\n",
        "        tokenizer.decode(\n",
        "            g,\n",
        "            skip_special_tokens=True,\n",
        "            clean_up_tokenization_spaces=True,\n",
        "        )\n",
        "        for g in batch[\"decoder_input_ids\"]\n",
        "    ]\n",
        "\n",
        "    preds = pre_process(preds)\n",
        "    labels = pre_process(labels)\n",
        "\n",
        "    for p, l in zip(preds, labels):\n",
        "        meteor_avg += meteor_score.single_meteor_score(p.split(\" \"), l.split(\" \"))\n",
        "        gleu_avg += gleu_score.sentence_gleu(p.split(\" \"), l.split(\" \"))\n",
        "        bleu_avg += bleu_score.sentence_bleu([p.split(\" \")], l.split(\" \"), weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    chrf(preds, labels)\n",
        "    rouge(preds, labels)\n",
        "    preds_collect.append(preds)\n",
        "    labels_collect.append(labels)\n",
        "\n",
        "print(\"------\")\n",
        "print(f\"Meteor: {meteor_avg / (4 * 113)}\")\n",
        "print(f\"GLEU: {gleu_avg / (4 * 113)}\")\n",
        "print(f\"BLEU: {bleu_avg / (4 * 113)}\")\n",
        "print(f\"CHRF: {chrf.compute().item()}\")\n",
        "print(f\"ROUGE: {rouge.compute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3eZkafpe71J"
      },
      "source": [
        "# Misc\n",
        "Below are some other useful chunks of code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMVz2qmLe71K"
      },
      "source": [
        "## Export prediction & labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iOPRt-v-tad"
      },
      "outputs": [],
      "source": [
        "print(labels_collect[0][1])\n",
        "print(preds_collect[0][1])\n",
        "\n",
        "label_final = [i for x in labels_collect for i in x]\n",
        "pred_final = [i for x in preds_collect for i in x]\n",
        "\n",
        "\n",
        "export = pd.DataFrame(\n",
        "    data=zip(label_final, pred_final), columns=[\"Labels\", \"Predictions\"]\n",
        ")\n",
        "export.to_json(\"output.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJhICYtQe71L"
      },
      "source": [
        "## Zip up Outputs\n",
        "Zip up the outputs & logs folder (with the models) to prepare them for exporting using zstd.\n",
        "\n",
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-K8XyRk4e71L"
      },
      "outputs": [],
      "source": [
        "!apt install tar zstd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVktmw5le71L"
      },
      "source": [
        "### Zipping up everything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1S0A_Cke71M"
      },
      "outputs": [],
      "source": [
        "!tar -c -I \"zstd -19 -T0\" -f \"mt5-thai-qg.tar.zst\" outputs/ lightning_logs/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "mT5_train_thai (7).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "599dbc988367df75f394341c14191e4f45ca195597a9b0aa004fda5844c2c20f"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
